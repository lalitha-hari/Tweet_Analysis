{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40W8L76VOqHU"
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Import Functions and Data](#0)\n",
    "- [1 - Logistic Regression](#1)\n",
    "    - [1.1 - Sigmoid](#1-1)\n",
    "        - [Exercise 1 - sigmoid](#ex-1)\n",
    "    - [1.2 - Cost function and Gradient](#1-2)\n",
    "        - [Exercise 2 - gradientDescent](#ex-2)\n",
    "- [2 - Extracting the Features](#2)\n",
    "    - [Exercise 3 - extract_features](#ex-3)\n",
    "- [3 - Training Your Model](#3)\n",
    "- [4 - Test your Logistic Regression](#4)\n",
    "    - [Exercise 4 - predict_tweet](#ex-4)\n",
    "    - [4.1 - Check the Performance using the Test Set](#4-1)\n",
    "        - [Exercise 5 - test_logistic_regression](#ex-5)\n",
    "- [5 - Error Analysis](#5)\n",
    "- [6 - Predict with your own Tweet](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDS5YvdoOqHU"
   },
   "source": [
    "<a name='0'></a>\n",
    "## Import Functions and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5709,
     "status": "ok",
     "timestamp": 1732606466226,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "O05fIJ99OqHU",
    "outputId": "3d0c2393-4916-4b7f-b665-6099f2d36933"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# run this cell to import nltk\n",
    "import nltk\n",
    "from os import getcwd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvSC56eXOqHV"
   },
   "source": [
    "#### Helper functions for data processing:\n",
    "* process_tweet: cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 418,
     "status": "ok",
     "timestamp": 1732606470637,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "85_svc7nOqHW"
   },
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPKunZGJOqHW"
   },
   "source": [
    "### Prepare the Data\n",
    "* The `twitter_samples` contains subsets of five thousand positive_tweets, five thousand negative_tweets, and the full set of 10,000 tweets.  \n",
    "    * If you used all three datasets, we would introduce duplicates of the positive tweets and negative tweets.  \n",
    "    * You will select just the five thousand positive tweets and five thousand negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 783,
     "status": "ok",
     "timestamp": 1732606475621,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "pQ1MRAS3OqHW"
   },
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "# print(len(all_positive_tweets))\n",
    "# print(len(all_negative_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApKWrYeEOqHW"
   },
   "source": [
    "* Train test split: 20% will be in the test set, and 80% in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1732606480214,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "N31aXGdfOqHX"
   },
   "outputs": [],
   "source": [
    "# split the data into two pieces, one for training and one for testing (validation set)\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7L_tDJ-_OqHX"
   },
   "source": [
    "* Create the numpy array of positive labels and negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1732606482862,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "_WjgrA_FOqHX"
   },
   "outputs": [],
   "source": [
    "# combine positive and negative labels\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1732606485546,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "J7qFVGtkOqHX",
    "outputId": "6460096c-cd05-402b-b6dc-ab21259fc48f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (8000, 1)\n",
      "test_y.shape = (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape train and test sets\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBJKU-CuOqHX"
   },
   "source": [
    "* Create the frequency dictionary by completing the build_freqs function: this counts how often a word in the 'corpus' (the entire set of tweets) was associated with a positive label '1' or a negative label '0', then builds the 'freqs' dictionary, where each key is the (word,label) tuple, and the value is the count of its frequency within the corpus of tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1732606489124,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "qRLfgdR8h81G"
   },
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"Build frequencies.\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        ys: an m x 1 array with the sentiment label of each tweet\n",
    "            (either 0 or 1)\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "        frequency\n",
    "    \"\"\"\n",
    "   \n",
    "    freqs = {}\n",
    "    # print(len(tweets))#8000\n",
    "\n",
    "    # print(len(ys))#8000\n",
    "    for i in range(len(tweets)):\n",
    "          y = ys[i][0]\n",
    "          #print(y) #1.0,0.0\n",
    "          tweet = tweets[i]\n",
    "          # print(tweet)\n",
    "          for word in process_tweet(tweet):\n",
    "              pair = (word, y)\n",
    "              # print(pair)\n",
    "              if pair in freqs:\n",
    "                  freqs[pair] += 1\n",
    "              else:\n",
    "                  freqs[pair] = 1\n",
    "         \n",
    "    # print(freqs)\n",
    "\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3985,
     "status": "ok",
     "timestamp": 1732606496641,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "Dn4Es2wrkCqN",
    "outputId": "f7c94140-e877-4d67-acb6-681b9cfd62d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 11427\n"
     ]
    }
   ],
   "source": [
    "# create frequency dictionary\n",
    "freqs = build_freqs(train_x, train_y)\n",
    "\n",
    "# check the output\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nJU9ZFVOqHY"
   },
   "source": [
    "#### Expected output\n",
    "```\n",
    "type(freqs) = <class 'dict'>\n",
    "len(freqs) = 11427\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUHAmptNOqHY"
   },
   "source": [
    "### Process Tweet\n",
    "The given function 'process_tweet' tokenizes the tweet into individual words, removes stop words and applies stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 447,
     "status": "ok",
     "timestamp": 1732606500593,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "5dXuGG6bOqHY",
    "outputId": "4aef6003-e7d5-42fb-e8c0-cfe7be8739ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of a positive tweet: \n",
      " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "This is an example of the processed version of the tweet: \n",
      " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "# test the function below\n",
    "print('This is an example of a positive tweet: \\n', train_x[0])\n",
    "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2uwrJ-ROqHY"
   },
   "source": [
    "#### Expected output\n",
    "```\n",
    "This is an example of a positive tweet:\n",
    " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
    "\n",
    "This is an example of the processes version:\n",
    " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X67WmHYjOqHY"
   },
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Logistic Regression\n",
    "\n",
    "<a name='1-1'></a>\n",
    "### 1.1 - Sigmoid\n",
    "You will learn to use logistic regression for text classification.\n",
    "* The sigmoid function is defined as:\n",
    "\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
    "\n",
    "It maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a probability.\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='https://drive.google.com/file/d/1bbsYdwwkA1LVkON7lafLd0CEnfLDxljA/view?usp=share_link' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:300px;height:200px;\" /> Figure 1 </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nq--F9l8OqHY"
   },
   "source": [
    "<a name='ex-1'></a>\n",
    "### Exercise 1 -  sigmoid\n",
    "Implement the sigmoid function.\n",
    "* You will want this function to work if z is a scalar as well as if it is an array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcs2qQowOqHY"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html\" > numpy.exp </a> </li>\n",
    "\n",
    "</ul>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 389,
     "status": "ok",
     "timestamp": 1732606506147,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "1X4s4WvHOqHY"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        z: is the input (can be a scalar or an array)\n",
    "    Output:\n",
    "        h: the sigmoid of z\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # calculate the sigmoid of z\n",
    "    h =  1 / (1 + np.exp(-z))\n",
    "    \n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1732606509902,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "4C4wWp_BOqHZ"
   },
   "outputs": [],
   "source": [
    "# Test cases for sigmoid function\n",
    "def test_sigmoid(target):\n",
    "    successful_cases = 0\n",
    "    failed_cases = []\n",
    "\n",
    "    test_cases = [\n",
    "        {\"name\": \"default_check\", \"input\": {\"z\": 0}, \"expected\": 0.5},\n",
    "        {\n",
    "            \"name\": \"positive_check\",\n",
    "            \"input\": {\"z\": 4.92},\n",
    "            \"expected\": 0.9927537604041685,\n",
    "        },\n",
    "        {\"name\": \"negative_check\", \"input\": {\"z\": -1}, \"expected\": 0.2689414213699951},\n",
    "        {\n",
    "            \"name\": \"larger_neg_check\",\n",
    "            \"input\": {\"z\": -20},\n",
    "            \"expected\": 2.0611536181902037e-09,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        result = target(**test_case[\"input\"])\n",
    "\n",
    "        try:\n",
    "            assert np.isclose(result, test_case[\"expected\"])\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": test_case[\"expected\"],\n",
    "                    \"got\": result,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong output from sigmoid function. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "    if len(failed_cases) == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(\"\\033[92m\", successful_cases, \" Tests passed\")\n",
    "        print(\"\\033[91m\", len(failed_cases), \" Tests failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1732606513499,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "EIK4tsfMOqHZ",
    "outputId": "f3e5c6f3-c675-43ad-ba77-6f621fc3e06a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "test_sigmoid(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da3Ee6nzOqHZ"
   },
   "source": [
    "#### Logistic Regression: Regression and a Sigmoid\n",
    "\n",
    "Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.\n",
    "\n",
    "Regression:\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "Note that the $\\theta$ values are \"weights\". If you took the deep learning specialization, we referred to the weights with the 'w' vector.  In this course, we're using a different variable $\\theta$ to refer to the weights.\n",
    "\n",
    "Logistic regression\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "We will refer to 'z' as the 'logits'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5W6AzoRCOqHZ"
   },
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - Cost function and Gradient\n",
    "\n",
    "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
    "* $m$ is the number of training examples\n",
    "* $y^{(i)}$ is the actual label of training example 'i'.\n",
    "* $h(z^{(i)})$ is the model's prediction for the training example 'i'.\n",
    "\n",
    "The loss function for a single training example is\n",
    "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
    "\n",
    "* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n",
    "* Note that when the model predicts 1 ($h(z(\\theta)) = 1$) and the label 'y' is also 1, the loss for that training example is 0.\n",
    "* Similarly, when the model predicts 0 ($h(z(\\theta)) = 0$) and the actual label is also 0, the loss for that training example is 0.\n",
    "* However, when the model prediction is close to 1 ($h(z(\\theta)) = 0.9999$) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. $-1 \\times (1 - 0) \\times log(1 - 0.9999) \\approx 9.2$ The closer the model prediction gets to 1, the larger the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1732606520724,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "ZJb-GtLyOqHZ",
    "outputId": "a9ff6516-535e-4570-a38e-e971762d372d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.210340371976294"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that when the model predicts close to 1, but the actual label is 0, the loss is a large positive value\n",
    "-1 * (1 - 0) * np.log(1 - 0.9999) # loss is about 9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-3CxMpuOqHZ"
   },
   "source": [
    "* Likewise, if the model predicts close to 0 ($h(z) = 0.0001$) but the actual label is 1, the first term in the loss function becomes a large number: $-1 \\times log(0.0001) \\approx 9.2$.  The closer the prediction is to zero, the larger the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1732606524317,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "379uX8FzOqHZ",
    "outputId": "bff1732c-c891-442b-9dd7-0e8931507376"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.210340371976182"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that when the model predicts close to 0 but the actual label is 1, the loss is a large positive value\n",
    "-1 * np.log(0.0001) # loss is about 9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeqUSRu4OqHa"
   },
   "source": [
    "#### Update the weights\n",
    "\n",
    "To update your weight vector $\\theta$, you will apply gradient descent to iteratively improve your model's predictions.  \n",
    "The gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n",
    "\n",
    "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x^{(i)}_j \\tag{5}$$\n",
    "* 'i' is the index across all 'm' training examples.\n",
    "* 'j' is the index of the weight $\\theta_j$, so $x^{(i)}_j$ is the feature associated with weight $\\theta_j$\n",
    "\n",
    "* To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n",
    "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
    "* The learning rate $\\alpha$ is a value that we choose to control how big a single update will be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUqbXaZjOqHa"
   },
   "source": [
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - gradientDescent\n",
    "Implement gradient descent function.\n",
    "* The number of iterations 'num_iters\" is the number of times that you'll use the entire training set.\n",
    "* For each iteration, you'll calculate the cost function using all training examples (there are 'm' training examples), and for all features.\n",
    "* Instead of updating a single weight $\\theta_i$ at a time, we can update all the weights in the column vector:  \n",
    "$$\\mathbf{\\theta} = \\begin{pmatrix}\n",
    "\\theta_0\n",
    "\\\\\n",
    "\\theta_1\n",
    "\\\\\n",
    "\\theta_2\n",
    "\\\\\n",
    "\\vdots\n",
    "\\\\\n",
    "\\theta_n\n",
    "\\end{pmatrix}$$\n",
    "* $\\mathbf{\\theta}$ has dimensions (n+1, 1), where 'n' is the number of features, and there is one more element for the bias term $\\theta_0$ (note that the corresponding feature value $\\mathbf{x_0}$ is 1).\n",
    "* The 'logits', 'z', are calculated by multiplying the feature matrix 'x' with the weight vector 'theta'.  $z = \\mathbf{x}\\mathbf{\\theta}$\n",
    "    * $\\mathbf{x}$ has dimensions (m, n+1)\n",
    "    * $\\mathbf{\\theta}$: has dimensions (n+1, 1)\n",
    "    * $\\mathbf{z}$: has dimensions (m, 1)\n",
    "* The prediction 'h', is calculated by applying the sigmoid to each element in 'z': $h(z) = sigmoid(z)$, and has dimensions (m,1).\n",
    "* The cost function $J$ is calculated by taking the dot product of the vectors 'y' and 'log(h)'.  Since both 'y' and 'h' are column vectors (m,1), transpose the vector to the left, so that matrix multiplication of a row vector with column vector performs the dot product.\n",
    "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
    "* The update of theta is also vectorized.  Because the dimensions of $\\mathbf{x}$ are (m, n+1), and both $\\mathbf{h}$ and $\\mathbf{y}$ are (m, 1), we need to transpose the $\\mathbf{x}$ and place it on the left in order to perform matrix multiplication, which then yields the (n+1, 1) answer we need:\n",
    "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjpnmChWOqHa"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>use numpy.dot for matrix multiplication.</li>\n",
    "    <li>To ensure that the fraction -1/m is a decimal value, cast either the numerator or denominator (or both), like `float(1)`, or write `1.` for the float version of 1. </li>\n",
    "</ul>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1732606528727,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "SsJCCpYMOqHa"
   },
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: your final weight vector\n",
    "    Hint: you might want to print the cost to make sure that it is going down.\n",
    "    \"\"\"\n",
    "   \n",
    "    # get 'm', the number of rows in matrix x\n",
    "    m = x.shape[0]\n",
    "\n",
    "    for i in range(0, num_iters):\n",
    "\n",
    "        # get z, the dot product of x and theta\n",
    "        z = np.dot(x, theta)\n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)\n",
    "\n",
    "        # calculate the cost function\n",
    "        J = -(1/m) * (np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1 - h)))\n",
    "\n",
    "        # update the weights theta\n",
    "        theta = theta - (alpha/m) * np.dot(x.T, (h - y))\n",
    "\n",
    "\n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1732606534304,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "lgRzzOV5leKu"
   },
   "outputs": [],
   "source": [
    "# Test cases for gradient descent function\n",
    "def test_gradientDescent(target):\n",
    "    successful_cases = 0\n",
    "    failed_cases = []\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"default_check\",\n",
    "            \"input\": {\n",
    "                \"random_seed\": 1,\n",
    "                \"input_dict\": {\n",
    "                    \"x\": np.array(\n",
    "                        [\n",
    "                            [1.00000000e00, 8.34044009e02, 1.44064899e03],\n",
    "                            [1.00000000e00, 2.28749635e-01, 6.04665145e02],\n",
    "                            [1.00000000e00, 2.93511782e02, 1.84677190e02],\n",
    "                            [1.00000000e00, 3.72520423e02, 6.91121454e02],\n",
    "                            [1.00000000e00, 7.93534948e02, 1.07763347e03],\n",
    "                            [1.00000000e00, 8.38389029e02, 1.37043900e03],\n",
    "                            [1.00000000e00, 4.08904499e02, 1.75623487e03],\n",
    "                            [1.00000000e00, 5.47751864e01, 1.34093502e03],\n",
    "                            [1.00000000e00, 8.34609605e02, 1.11737966e03],\n",
    "                            [1.00000000e00, 2.80773877e02, 3.96202978e02],\n",
    "                        ]\n",
    "                    ),\n",
    "                    \"y\": np.array(\n",
    "                        [\n",
    "                            [1.0],\n",
    "                            [1.0],\n",
    "                            [0.0],\n",
    "                            [1.0],\n",
    "                            [1.0],\n",
    "                            [1.0],\n",
    "                            [0.0],\n",
    "                            [0.0],\n",
    "                            [0.0],\n",
    "                            [1.0],\n",
    "                        ]\n",
    "                    ),\n",
    "                    \"theta\": np.zeros((3, 1)),\n",
    "                    \"alpha\": 1e-8,\n",
    "                    \"num_iters\": 700,\n",
    "                },\n",
    "            },\n",
    "            \"expected\": {\n",
    "                \"J\": 0.6709497038162118,\n",
    "                \"theta\": np.array(\n",
    "                    [[4.10713435e-07], [3.56584699e-04], [7.30888526e-05]]\n",
    "                ),\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"larger_check\",\n",
    "            \"input\": {\n",
    "                \"random_seed\": 2,\n",
    "                \"input_dict\": {\n",
    "                    \"x\": np.array(\n",
    "                        [\n",
    "                            [1.0, 435.99490214, 25.92623183, 549.66247788],\n",
    "                            [1.0, 435.32239262, 420.36780209, 330.334821],\n",
    "                            [1.0, 204.64863404, 619.27096635, 299.65467367],\n",
    "                            [1.0, 266.8272751, 621.13383277, 529.14209428],\n",
    "                            [1.0, 134.57994534, 513.57812127, 184.43986565],\n",
    "                            [1.0, 785.33514782, 853.97529264, 494.23683738],\n",
    "                            [1.0, 846.56148536, 79.64547701, 505.24609012],\n",
    "                            [1.0, 65.28650439, 428.1223276, 96.53091566],\n",
    "                            [1.0, 127.1599717, 596.74530898, 226.0120006],\n",
    "                            [1.0, 106.94568431, 220.30620707, 349.826285],\n",
    "                            [1.0, 467.78748458, 201.74322626, 640.40672521],\n",
    "                            [1.0, 483.06983555, 505.23672002, 386.89265112],\n",
    "                            [1.0, 793.63745444, 580.00417888, 162.2985985],\n",
    "                            [1.0, 700.75234661, 964.55108009, 500.00836117],\n",
    "                            [1.0, 889.52006395, 341.61365267, 567.14412763],\n",
    "                            [1.0, 427.5459633, 436.74726303, 776.559185],\n",
    "                            [1.0, 535.6041735, 953.74222694, 544.20816015],\n",
    "                            [1.0, 82.09492228, 366.34240168, 850.850504],\n",
    "                            [1.0, 406.27504305, 27.20236589, 247.177239],\n",
    "                            [1.0, 67.14437074, 993.85201142, 970.58031338],\n",
    "                        ]\n",
    "                    ),\n",
    "                    \"y\": np.array(\n",
    "                        [\n",
    "                            [1.0],\n",
    "                            [1.0],\n",
    "                            [1.0],\n",
    "                            [0.0],\n",
    "                            [0.0],\n",
    "                            [1.0],\n",
    "                            [0.0],\n",
    "                            [0.0],\n",
    "                            [1.0],\n",
    "                            [0.0],\n",
    "                            [1.0],\n",
    "                            [0.0],\n",
    "                            [0.0],\n",
    "                            [0.0],\n",
    "                            [1.0],\n",
    "                            [1.0],\n",
    "                            [0.0],\n",
    "                            [0.0],\n",
    "                            [1.0],\n",
    "                            [0.0],\n",
    "                        ]\n",
    "                    ),\n",
    "                    \"theta\": np.zeros((4, 1)),\n",
    "                    \"alpha\": 1e-4,\n",
    "                    \"num_iters\": 30,\n",
    "                },\n",
    "            },\n",
    "            \"expected\": {\n",
    "                \"J\": 6.5044107216556135,\n",
    "                \"theta\": np.array(\n",
    "                    [\n",
    "                        [9.45211976e-05],\n",
    "                        [2.40577958e-02],\n",
    "                        [-1.77876847e-02],\n",
    "                        [1.35674845e-02],\n",
    "                    ]\n",
    "                ),\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        # Setting the random seed for reproducibility\n",
    "        result_J, result_theta = target(**test_case[\"input\"][\"input_dict\"])\n",
    "\n",
    "        try:\n",
    "            assert isinstance(result_J, float)\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\"name\": test_case[\"name\"], \"expected\": float, \"got\": type(result_J),}\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong output type for loss function. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            assert np.isclose(result_J, test_case[\"expected\"][\"J\"])\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": test_case[\"expected\"][\"J\"],\n",
    "                    \"got\": result_J,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong output for the loss function. Check how you are implementing the matrix multiplications. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            assert result_theta.shape == test_case[\"input\"][\"input_dict\"][\"theta\"].shape\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": test_case[\"input\"][\"input_dict\"][\"theta\"].shape,\n",
    "                    \"got\": result_theta.shape,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong shape for weights matrix theta. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            assert np.allclose(\n",
    "                np.squeeze(result_theta), np.squeeze(test_case[\"expected\"][\"theta\"]),\n",
    "            )\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": test_case[\"expected\"][\"theta\"],\n",
    "                    \"got\": result_theta,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong values for weight's matrix theta. Check how you are updating the matrix of weights. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "    if len(failed_cases) == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(\"\\033[92m\", successful_cases, \" Tests passed\")\n",
    "        print(\"\\033[91m\", len(failed_cases), \" Tests failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 655,
     "status": "ok",
     "timestamp": 1732606540086,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "Nw9CiQw0OqHb",
    "outputId": "8970fd34-0e57-467b-c2a1-245e02088a87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-74e1d42e1029>:33: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  J = float(J)\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "test_gradientDescent(gradientDescent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdST-tmDOqHb"
   },
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Extracting the Features\n",
    "\n",
    "* Given a list of tweets, extract the features and store them in a matrix. You will extract two features.\n",
    "    * The first feature is the number of positive words in a tweet.\n",
    "    * The second feature is the number of negative words in a tweet.\n",
    "* Then train your logistic regression classifier on these features.\n",
    "* Test the classifier on a validation set.\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - extract_features\n",
    "Implement the extract_features function.\n",
    "* This function takes in a single tweet.\n",
    "* Process the tweet using the imported `process_tweet` function and save the list of tweet words.\n",
    "* Loop through each word in the list of processed words\n",
    "    * For each word, check the 'freqs' dictionary for the count when that word has a positive '1' label. (Check for the key (word, 1.0)\n",
    "    * Do the same for the count for when the word is associated with the negative label '0'. (Check for the key (word, 0.0).)\n",
    "\n",
    "**Note:** In the implementation instructions provided above, the prediction of being positive or negative depends on feature vector which counts-in duplicate words - this is different from what you have seen in the lecture videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3lF_muMOqHb"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Make sure you handle cases when the (word, label) key is not found in the dictionary. </li>\n",
    "    <li> Search the web for hints about using the 'get' function of a Python dictionary.  Here is an <a href=\"https://www.programiz.com/python-programming/methods/dictionary/get\" > example </a> </li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1732606629118,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "JUKXYF-NOqHc"
   },
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs, process_tweet=process_tweet):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        tweet: a string containing one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output:\n",
    "        x: a feature vector of dimension (1,3)\n",
    "    \"\"\"\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "\n",
    "    # 3 elements for [bias, positive, negative] counts\n",
    "    x = np.zeros(3)\n",
    "\n",
    "    # bias term is set to 1\n",
    "    x[0] = 1\n",
    "\n",
    " \n",
    "\n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "\n",
    "        # increment the word count for the positive label 1\n",
    "        x[1] += freqs.get((word, 1.0), 0)\n",
    "\n",
    "        # increment the word count for the negative label 0\n",
    "        x[2] += freqs.get((word, 0.0), 0)\n",
    "\n",
    "    \n",
    "\n",
    "    x = x[None, :]  # adding batch dimension for further processing, which converts x into 2D array\n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 439,
     "status": "ok",
     "timestamp": 1732606895291,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "gtmbLLJbl3AH"
   },
   "outputs": [],
   "source": [
    "# Test cases for extract_features\n",
    "def test_extract_features(target, freqs):\n",
    "    successful_cases = 0\n",
    "    failed_cases = []\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"default_check\",\n",
    "            \"input\": {\n",
    "                \"tweet\": \"#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\",\n",
    "                \"freqs\": freqs,\n",
    "            },\n",
    "            \"expected\": np.array(\n",
    "                [[1.00e00, 3.133e03, 6.10e01]]\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"unk_words_check\",\n",
    "            \"input\": {\"tweet\": \"blorb bleeeeb bloooob\", \"freqs\": freqs},\n",
    "            \"expected\": np.array([[1.0, 0.0, 0.0]]),\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"good_words_check\",\n",
    "            \"input\": {\"tweet\": \"Hello world! All's good!\", \"freqs\": freqs},\n",
    "            \"expected\": np.array([[1.0, 263.0, 106.0]]),\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"bad_words_check\",\n",
    "            \"input\": {\"tweet\": \"It is so sad!\", \"freqs\": freqs},\n",
    "            \"expected\": np.array([[1.0, 5.0, 100.0]]),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        result = target(**test_case[\"input\"])\n",
    "\n",
    "        try:\n",
    "            assert result.shape == test_case[\"expected\"].shape\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": test_case[\"expected\"].shape,\n",
    "                    \"got\": result.shape,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong output shape. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            assert np.allclose(result, test_case[\"expected\"])\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": test_case[\"expected\"],\n",
    "                    \"got\": result,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong output values. Check how you are computing the positive or negative word count. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "    if len(failed_cases) == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(\"\\033[92m\", successful_cases, \" Tests passed\")\n",
    "        print(\"\\033[91m\", len(failed_cases), \" Tests failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 442,
     "status": "ok",
     "timestamp": 1732606900593,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "QtNTNCrnOqHd",
    "outputId": "36ca4b1d-6e93-4e55-e945-157dca77b3ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "test_extract_features(extract_features, freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWpkGF0JOqHd"
   },
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Training Your Model\n",
    "\n",
    "To train the model:\n",
    "* Stack the features for all training examples into a matrix X.\n",
    "* Call `gradientDescent`, which you've implemented above.\n",
    "\n",
    "This section is given to you.  Please read it for understanding and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6056,
     "status": "ok",
     "timestamp": 1732607647254,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "FRTMeIjpOqHd",
    "outputId": "a3e3ced4-3f38-4d03-b159-efabda2efeea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.22521264.\n",
      "The resulting vector of weights is [6e-08, 0.0005382, -0.0005583]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-74e1d42e1029>:33: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  J = float(J)\n"
     ]
    }
   ],
   "source": [
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extract_features(train_x[i], freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y\n",
    "\n",
    "# Apply gradient descent\n",
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vowEzH0EOqHd"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```\n",
    "The cost after training is 0.22522315.\n",
    "The resulting vector of weights is [6e-08, 0.00053818, -0.0005583]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Mwn_OA0OqHd"
   },
   "source": [
    "<a name='4'></a>\n",
    "## 4 -  Test your Logistic Regression\n",
    "\n",
    "It is time for you to test your logistic regression function on some new input that your model has not seen before.\n",
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - predict_tweet\n",
    "Implement `predict_tweet`.\n",
    "Predict whether a tweet is positive or negative.\n",
    "\n",
    "* Given a tweet, process it, then extract the features.\n",
    "* Apply the model's learned weights on the features to get the logits.\n",
    "* Apply the sigmoid to the logits to get the prediction (a value between 0 and 1).\n",
    "\n",
    "$$y_{pred} = sigmoid(\\mathbf{x} \\cdot \\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1732608030570,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "y1I6mCndOqHd"
   },
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output:\n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    \"\"\"\n",
    "   \n",
    "\n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet, freqs)\n",
    "\n",
    "    # make the prediction using x and theta\n",
    "    z = np.dot(x, theta)\n",
    "    y_pred = sigmoid(z)\n",
    "\n",
    " \n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1732608061660,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "3evmk_d1mROP"
   },
   "outputs": [],
   "source": [
    "# Test cases for predict_tweet\n",
    "def test_predict_tweet(target, freqs, theta):\n",
    "    successful_cases = 0\n",
    "    failed_cases = []\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"default_check1\",\n",
    "            \"input\": {\"tweet\": \"I am happy\", \"freqs\": freqs, \"theta\": theta},\n",
    "            \"expected\": np.array([[0.5192746]]),\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"default_check2\",\n",
    "            \"input\": {\"tweet\": \"I am bad\", \"freqs\": freqs, \"theta\": theta},\n",
    "            \"expected\": np.array([[0.49434685]]),\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"default_check3\",\n",
    "            \"input\": {\n",
    "                \"tweet\": \"this movie should have been great\",\n",
    "                \"freqs\": freqs,\n",
    "                \"theta\": theta,\n",
    "            },\n",
    "            \"expected\": np.array([[0.5159792]]),\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"default_check5\",\n",
    "            \"input\": {\"tweet\": \"It is a good day\", \"freqs\": freqs, \"theta\": theta,},\n",
    "            \"expected\": np.array([[0.52320595]]),\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"default_check6\",\n",
    "            \"input\": {\"tweet\": \"It is a bad bad day\", \"freqs\": freqs, \"theta\": theta,},\n",
    "            \"expected\": np.array([[0.49780224]]),\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"default_check7\",\n",
    "            \"input\": {\n",
    "                \"tweet\": \"It is a good day\",\n",
    "                \"freqs\": freqs,\n",
    "                \"theta\": np.array([[5.0000e-04], [-3.4e-02], [3.2e-02]]),\n",
    "            },\n",
    "            \"expected\": np.array([[0.00147813]]),\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"default_check8\",\n",
    "            \"input\": {\n",
    "                \"tweet\": \"It is a bad bad day\",\n",
    "                \"freqs\": freqs,\n",
    "                \"theta\": np.array([[5.0000e-04], [-3.4e-02], [3.2e-02]]),\n",
    "            },\n",
    "            \"expected\": np.array([[0.45673348]]),\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"default_check9\",\n",
    "            \"input\": {\n",
    "                \"tweet\": \"this movie should have been great\",\n",
    "                \"freqs\": freqs,\n",
    "                \"theta\": np.array([[5.0000e-04], [-3.4e-02], [3.2e-02]]),\n",
    "            },\n",
    "            \"expected\": np.array([[0.01561938]]),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        result = target(**test_case[\"input\"])\n",
    "\n",
    "        try:\n",
    "            assert result.shape == test_case[\"expected\"].shape\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": test_case[\"expected\"].shape,\n",
    "                    \"got\": result.shape,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong output shape. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            assert np.allclose(result, test_case[\"expected\"])\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": test_case[\"expected\"],\n",
    "                    \"got\": result,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong predicted values. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "    if len(failed_cases) == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(\"\\033[92m\", successful_cases, \" Tests passed\")\n",
    "        print(\"\\033[91m\", len(failed_cases), \" Tests failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1732608067060,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "Ds6cphyCOqHd",
    "outputId": "8d3b2d3d-6c51-4b14-9e27-c91f346c78c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "test_predict_tweet(predict_tweet, freqs, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5w-bufGOqHd"
   },
   "source": [
    "<a name='4-1'></a>\n",
    "### 4.1 -  Check the Performance using the Test Set\n",
    "After training your model using the training set above, check how your model might perform on real, unseen data, by testing it against the test set.\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### Exercise 5 - test_logistic_regression\n",
    "Implement `test_logistic_regression`.\n",
    "* Given the test data and the weights of your trained model, calculate the accuracy of your logistic regression model.\n",
    "* Use your 'predict_tweet' function to make predictions on each tweet in the test set.\n",
    "* If the prediction is > 0.5, set the model's classification 'y_hat' to 1, otherwise set the model's classification 'y_hat' to 0.\n",
    "* A prediction is accurate when the y_hat equals the test_y.  Sum up all the instances when they are equal and divide by m.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAx9zyBDOqHd"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use np.asarray() to convert a list to a numpy array</li>\n",
    "    <li>Use numpy.squeeze() to make an (m,1) dimensional array into an (m,) array </li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1732608261245,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "KBJc_Xz8OqHe"
   },
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, test_y, freqs, theta, predict_tweet=predict_tweet):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: a list of tweets\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: weight vector of dimension (3, 1)\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "\n",
    "    for tweet in test_x:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "\n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1.0)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0.0)\n",
    "    y_hat = np.array(y_hat)\n",
    "    test_y = np.squeeze(test_y)\n",
    "\n",
    "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
    "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
    "    accuracy = np.sum(y_hat == test_y) / len(test_y)\n",
    "\n",
    "    \n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1375,
     "status": "ok",
     "timestamp": 1732608267227,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "PwqwJrVaOqHe",
    "outputId": "19afee80-c3bf-4c14-f152-5a30fba47014"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.9950\n"
     ]
    }
   ],
   "source": [
    "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
    "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azYzIWxBOqHe"
   },
   "source": [
    "#### Expected Output:\n",
    "```0.9950```  \n",
    "Pretty good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1732608370559,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "dKfB6rxamsgn"
   },
   "outputs": [],
   "source": [
    "# Unit tests for test_logistic_regression\n",
    "def unittest_test_logistic_regression(target, freqs, theta):\n",
    "    successful_cases = 0\n",
    "    failed_cases = []\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"default_check1\",\n",
    "            \"input\": {\n",
    "                \"test_x\": [\n",
    "                    \"Bro:U wan cut hair anot,ur hair long Liao bo\\nMe:since ord liao,take it easy lor treat as save $ leave it longer :)\\nBro:LOL Sibei xialan\",\n",
    "                    \"@heyclaireee is back! thnx God!!! i'm so happy :)\",\n",
    "                    \"@BBCRadio3 thought it was my ears which were malfunctioning, thank goodness you cleared that one up with an apology :-)\",\n",
    "                    \"@HumayAG 'Stuck in the centre right with you. Clowns to the right, jokers to the left...' :) @orgasticpotency @ahmedshaheed @AhmedSaeedGahaa\",\n",
    "                    \"Happy Friday :-) http://t.co/iymPIlWXFY\",\n",
    "                    \"I wanna change my avi but uSanele :(\",\n",
    "                    \"MY PUPPY BROKE HER FOOT :(\",\n",
    "                    \"where's all the jaebum baby pictures :((\",\n",
    "                    \"But but Mr Ahmad Maslan cooks too :( https://t.co/ArCiD31Zv6\",\n",
    "                    \"@eawoman As a Hull supporter I am expecting a misserable few weeks :-(\",\n",
    "                ],\n",
    "                \"test_y\": np.array(\n",
    "                    [\n",
    "                        [1.0],\n",
    "                        [1.0],\n",
    "                        [1.0],\n",
    "                        [1.0],\n",
    "                        [1.0],\n",
    "                        [0.0],\n",
    "                        [0.0],\n",
    "                        [0.0],\n",
    "                        [0.0],\n",
    "                        [0.0],\n",
    "                    ]\n",
    "                ),\n",
    "                \"freqs\": freqs,\n",
    "                \"theta\": theta,\n",
    "            },\n",
    "            \"expected\": 1.0,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"default_check1\",\n",
    "            \"input\": {\n",
    "                \"test_x\": [\n",
    "                    \"Bro:U wan cut hair anot,ur hair long Liao bo\\nMe:since ord liao,take it easy lor treat as save $ leave it longer :)\\nBro:LOL Sibei xialan\",\n",
    "                    \"@heyclaireee is back! thnx God!!! i'm so happy :)\",\n",
    "                    \"@BBCRadio3 thought it was my ears which were malfunctioning, thank goodness you cleared that one up with an apology :-)\",\n",
    "                    \"@HumayAG 'Stuck in the centre right with you. Clowns to the right, jokers to the left...' :) @orgasticpotency @ahmedshaheed @AhmedSaeedGahaa\",\n",
    "                    \"Happy Friday :-) http://t.co/iymPIlWXFY\",\n",
    "                    \"I wanna change my avi but uSanele :(\",\n",
    "                    \"MY PUPPY BROKE HER FOOT :(\",\n",
    "                    \"where's all the jaebum baby pictures :((\",\n",
    "                    \"But but Mr Ahmad Maslan cooks too :( https://t.co/ArCiD31Zv6\",\n",
    "                    \"@eawoman As a Hull supporter I am expecting a misserable few weeks :-(\",\n",
    "                ],\n",
    "                \"test_y\": np.array(\n",
    "                    [\n",
    "                        [1.0],\n",
    "                        [1.0],\n",
    "                        [1.0],\n",
    "                        [1.0],\n",
    "                        [1.0],\n",
    "                        [0.0],\n",
    "                        [0.0],\n",
    "                        [0.0],\n",
    "                        [0.0],\n",
    "                        [0.0],\n",
    "                    ]\n",
    "                ),\n",
    "                \"freqs\": freqs,\n",
    "                \"theta\": np.array([[5.0000e-04], [-3.4e-02], [3.2e-02]]),\n",
    "            },\n",
    "            \"expected\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        result = target(**test_case[\"input\"])\n",
    "\n",
    "        try:\n",
    "            assert isinstance(result, np.float64)\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": np.float64,\n",
    "                    \"got\": type(result),\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong output type. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            assert np.isclose(result, test_case[\"expected\"])\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": test_case[\"expected\"],\n",
    "                    \"got\": result,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong accuracy value. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "    if len(failed_cases) == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(\"\\033[92m\", successful_cases, \" Tests passed\")\n",
    "        print(\"\\033[91m\", len(failed_cases), \" Tests failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tWPYVg-sOqHe",
    "outputId": "90e096fa-1b7f-4b20-9aa5-6025d5e7987b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "unittest_test_logistic_regression(test_logistic_regression, freqs, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OV-iMxhZOqHe"
   },
   "source": [
    "<a name='5'></a>\n",
    "## 5 -  Error Analysis\n",
    "\n",
    "In this part you will see some tweets that your model misclassified. Why do you think the misclassifications happened? Specifically what kind of tweets does your model misclassify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1167,
     "status": "ok",
     "timestamp": 1732608502583,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "w8OBmimAOqHe",
    "outputId": "87eda2cc-8d61-4407-f653-3b350e2778c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Predicted Tweet\n",
      "THE TWEET IS: @MarkBreech Not sure it would be good thing 4 my bottom daring 2 say 2 Miss B but Im gonna be so stubborn on mouth soaping ! #NotHavingit :p\n",
      "THE PROCESSED TWEET IS: ['sure', 'would', 'good', 'thing', '4', 'bottom', 'dare', '2', 'say', '2', 'miss', 'b', 'im', 'gonna', 'stubborn', 'mouth', 'soap', 'nothavingit', ':p']\n",
      "1\t0.48942981\tb'sure would good thing 4 bottom dare 2 say 2 miss b im gonna stubborn mouth soap nothavingit :p'\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots\n",
      "http://t.co/UGQzOx0huu\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48418981\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/aOKldo3GMj http://t.co/xWCM9qyRG5\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48418981\tb\"i'm play brain dot braindot\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-6eb8e65380e7>:9: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/R2JBO8iNww http://t.co/ow5BBwdEMY\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48418981\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: off to the park to get some sunlight : )\n",
      "THE PROCESSED TWEET IS: ['park', 'get', 'sunlight']\n",
      "1\t0.49636406\tb'park get sunlight'\n",
      "THE TWEET IS: @msarosh Uff Itna Miss karhy thy ap :p\n",
      "THE PROCESSED TWEET IS: ['uff', 'itna', 'miss', 'karhi', 'thi', 'ap', ':p']\n",
      "1\t0.48250522\tb'uff itna miss karhi thi ap :p'\n",
      "THE TWEET IS: @phenomyoutube u probs had more fun with david than me : (\n",
      "THE PROCESSED TWEET IS: ['u', 'prob', 'fun', 'david']\n",
      "0\t0.50988296\tb'u prob fun david'\n",
      "THE TWEET IS: pats jay : (\n",
      "THE PROCESSED TWEET IS: ['pat', 'jay']\n",
      "0\t0.50040366\tb'pat jay'\n",
      "THE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
      "THE PROCESSED TWEET IS: ['belov', 'grandmoth']\n",
      "0\t0.50000002\tb'belov grandmoth'\n",
      "THE TWEET IS: Sr. Financial Analyst - Expedia, Inc.: (#Bellevue, WA) http://t.co/ktknMhvwCI #Finance #ExpediaJobs #Job #Jobs #Hiring\n",
      "THE PROCESSED TWEET IS: ['sr', 'financi', 'analyst', 'expedia', 'inc', 'bellevu', 'wa', 'financ', 'expediajob', 'job', 'job', 'hire']\n",
      "0\t0.50648699\tb'sr financi analyst expedia inc bellevu wa financ expediajob job job hire'\n"
     ]
    }
   ],
   "source": [
    "# Some error analysis done for you\n",
    "print('Label Predicted Tweet')\n",
    "for x,y in zip(test_x,test_y):\n",
    "    y_hat = predict_tweet(x, freqs, theta)\n",
    "\n",
    "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
    "        print('THE TWEET IS:', x)\n",
    "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
    "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDHXFDWpOqHe"
   },
   "source": [
    "Later in this specialization, we will see how we can use deeplearning to improve the prediction performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U1bn4DzOqHe"
   },
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Predict with your own Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 387,
     "status": "ok",
     "timestamp": 1732608514855,
     "user": {
      "displayName": "Hari Venkata Lalitha Parameswari",
      "userId": "11516700403970931193"
     },
     "user_tz": -330
    },
    "id": "eyzmojhfOqHe",
    "outputId": "22955e15-a64f-4137-d50b-22a4c5344d7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ridicul', 'bright', 'movi', 'plot', 'terribl', 'sad', 'end']\n",
      "[[0.48125421]]\n",
      "Negative sentiment\n"
     ]
    }
   ],
   "source": [
    "# Feel free to change the tweet below\n",
    "my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n",
    "print(process_tweet(my_tweet))\n",
    "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
    "print(y_hat)\n",
    "if y_hat > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else:\n",
    "    print('Negative sentiment')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
